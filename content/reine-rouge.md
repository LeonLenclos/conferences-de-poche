Un bon exemple de question sans réponse, c'est une question souvent posé par les enfants aux adultes, c'est la question "jusqu'à combien tu sais compter ?". A priori, il n'y a pas de limite à jusqu'à combien je sais compter. Et je pourrai être tenté de répondre "Je sais compter jusqu'à l'infini". Mais en réalité, personne ne peut prétendre pouvoir compter juqu'à l'infini.

En réalité c'est une question très embarassante. Beaucoup plus embarassante à mes yeux que d'autres questions d'enfants réputées pour être embarassante. Comme par exemple "comment on fait les bébés" ou "est-ce que tu as déjà pris de la drogue".

Quand on me pose la question de jusqu'à combien je sais compter, ma stratégie en général c'est de détourner la conversation vers un autre question similaire, mais à laquelle je sais répondre. Par exemple, jusqu'à combien je sais compter sur mes doigts. Compter sur les doigts de sa main ça s'appelle la dactylonomie, et c'est vraiment un sujet passionnant.

En france on apprend en général à compter sur ses mains, en dépliant les doigts uns par uns en partant du pouce. Avec une main on peut en général aller jusqu'à 5. Et vu qu'on a deux mains, on peut aller jusqu'à 10. C'est la façon la plus primitive de compter sur ses doigts. C'est surment comme ça que comptaient les premiers humains qui ont compté sur leur doigts. Et c'est surement pour ça qu'on utilise la base 10 (le système décimal) pour remprésenter les nombres dans la vie de tous les jours.

![Mains qui comptent "à la française"](img/reine-rouge-01.tiff)

Cette technique, qu'on appelle la technique française, a un énorme défaut, elle est extrêmement inconfortable lorsqu'on atteind le nombre 4. Et on a un gros problème avec ça en France, un problème de santé publique. Avec un nombre de tendinite à l'auriculaire beaucoup plus élevé qu'ailleur.

Dans d'autres endroits on a des manières beaucoup moins douloureuse de compter jusqu'à 5. Par exemple en partant de l'index et en finissant par le pouce, En partant de la main ouverte, et en fermant les doigts uns par uns ou encore en partant du pouce mais en finissant par l'oriculaire.

Et donc tout ça là, c'est très bien pour compter publiquement, si je veux que la personne en fasse sache ou j'en suis. Mais si je dois compter pour moi-même en général je vais utiliser une autre technique qui s'appelle la technique décimale à deux chiffre. À deux chiffres ça veut dire que chacune de mes deux mains va représenter un chiffre. La main gauche pour le chiffre des dizaines et la main droite pour le chiffre des unités. Décimale ça veut dire que chacun de ces chiffre est compris entre 0 et 9. Donc en gros j'associe un chiffre entre 0 et 9 à chacune de mes phalanges, dans le même ordre que sur un clavier de calculatrice. Et pour compter, je déplace mes pouces sur mes phalanges. C'est très pratique et très intuitif. Et ça permet de compter jusqu'à 100.

![Mains qui comptent en décimal à deux chiffre](img/reine-rouge-02.tiff)

Dans les cas ou je dois compter risque de dépasser la centaine, j'utilise une dernière technique, la technique binaire à 10 chiffres. À 10 chiffres ça veut dire que chacun de mes doigts va représenter un chiffre. Binaire ça veut dire que chacun de ces chiffre peut être soit 0 soit 1. Cette technique est un peu plus compliquée, mais elle a un énorme avantage, elle permet de compter au delà de 1000. Pour ceux que ça interesse je vais rapidement expliquer comment ça fonctionne. On associe à chaque doigt une puissance de 2 (1, 2, 4, 8, 16, 32, 64, 128, 256, 512). Et pour savoir ou j'en suis dans le compte, je n'ai qu'a aditionner les nombre correspondants à mes doigts levés.

![Mains qui comptent en binaire à dix chiffres](img/reine-rouge-03.tiff)

Vous avez surement entendu parlé de cette technique, elle est de plus en plus utilisée. Là, il y a quelques jours, j'étais chez une amie qui est tatoueuse et elle me racontais que de plus en plus de gens viennent se faire tatouer les puissances de 2 sur le bout des doigts pour pouvoir plus facilement compter en binaire.

Bon, une fois que j'en arrive là dans l'explication de jusqu'à combien je sais compter sur les doigts, en général mon interlocuteur à totalement oublié sa question de départ qui était, je vous le rapelle, "jusqu'à combien je sais compter" (tout court). Mamheureusement, moi cette question ne va pas me quitter de si tôt.

---

Par exemple, dans l'histoire, c'est quoi le plus loin qu'on est allé dans le décompte des nombres entiers ? Dans le guiness des records, le reccord du plus grand nombre compté à voix haute il est détenu par un Américain qui s'appelle Jeremy Harper et qui a compté jusqu'à 1 000 000 en une centaine de jour. Et c'est assez étonnant, mais ils ne mentionnent pas du tout Opalka.

Opalka en gros c'est un peintre qui un jour a décidé de tout arrêter pour consacrer tout son temps au décompte des nombres de 0 à l'infini. Et donc il comptait à voix haute en s'enregistrant sur un dictaphone. Et en même temps il écrivait les nombres en tout petit sur des tableaux. Avec de la peinture à l'huile blanche (du blanc de titane). Bon malheureusement Opalka il est mort avant d'arriver à l'infini, mais il a quand même réussi à compter jusqu'à plus de 5 millions ! 5 fois plus que Jeremy Harper.

Et alors quand j'ai écris au Guiness des records à ce sujet, ils m'ont répondu qu'ils connaissaient bien le cas Opalka. Mais que son reccord ne pouvait pas être validé, parce qu'il y avait une erreur dans le comptage. Et en effet, c'est connu, sur un des tableau d'Opalka il manque un nombre. Un jour il était un peu fatigué et il est passé directement de 2345831 à 2345833. Mais franchement, c'est pas très grâve non ? Surtout que Jeremy Harper il n'a pas oublié de nombre mais contrairement à Opalka, il comptait pas de tête. Il était assisté par un ordinateur. Il avait un écran devant lui avec les nombres qui défilaient automatiquement. Et ça entre nous c'est un peu de la triche. Parce que compter pour un ordinateur, c'est vraiment facile. Compter c'est même ce que les ordinateurs savent faire de mieux. Je vais même aller plus loin, en vérité, compter c'est la seule chose que les ordinateurs savent faire.

Et c'est ça qu'on appelle le numérique quand on parle des ordinateurs. Numérique ça veut dire quoi ? Ça veut juste dire nombre. Parce qu'un ordinateur en fait, c'estt juste un nombre. Et quand on dit "le digital" pour parler des ordinateurs. Digital ça veux dire quoi ? Ça juste dire doigt (comme dans empruntes digitales) parce que en fait un ordinateur c'est juste une machine qui compte sur ses doigts en binaire. Rien de plus. Enfin, rien de plus à part que elle compte très vite, et qu'elle a beaucoup de doigts.

Rien que mon mon téléphone portable, mon ordinateur de poche, il fait 128 Go. 128 giga octets, ça veut dire qu'il peut compter sur 128 milliards de mains. Et les mains des ordinateurs c'est des octets, Ce qui veut dire qu'ils ont 8 doigts par mains. Et je sais pas vous, mais moi ça me fais toujours un peu bizarre de savoir que j'ai 128 milliards de mains à 8 doigts qui comptent en continu dans ma poche.

Les ordinateurs ont pas toujouré été aussi petit hein ! Si ils sont si petits aujourd'hui c'est principalement à cause de la lois de Moore. La lois de Moore pour résumer, ça dit que tous les deux ans on a besoin de deux fois moins de place pour construire un ordinateur de même puissance. Donc si mon ordinateur fait cette taille :

![Un objet de taille 1](img/reine-rouge-04.tiff)

Il y a deux ans, pour la même puissance, il aurait du faire le double :

![Un objet de taille 2](img/reine-rouge-05.tiff)

Deux ans avant, le double du double :

![Un objet de taille 4](img/reine-rouge-06.tiff)

Deux ans avant, le double du double du double :

![Un objet de taille 8](img/reine-rouge-07.tiff)

Et ainsi de suite :

![Un objet de taille 16](img/reine-rouge-08.tiff)

Donc la je m'arrête parce que je n'ai plus de place, mais vous avez compris le principe, c'est exponentiel, ça augmente très vite. Et si je répétais l'opérarion 50 fois, qu'est-ce qu'on découvrirait ? On découvrirait qu'il y a 100 ans, pour faire un ordinateur aussi puissant que mon ordinateur de poche il fallait un immeuble de 60 étages qui occupe toute la superficie de la ville de Toulouse.

Et je trouve que ça explique assez bien pourquoi, à cette époque là, personne n'avais envie de construire des ordinateurs. Parce construire un ordinateur de la taille de Toulouse, déjà il te faut pas mal de place, et puis surtout il te faut du temps. Et faudrait pas que la construction de l'ordinateur te prenne trop longtemps. Parce que si par exemple ça te prend 100 ans, au bout de 100 ans, le jour ou tu as fini ta ville-ordinateur, tu es content, tu sors faire un tour pour en parler autour de toi, et là tu t'apperçoit de quoi ? Que tout le monde à dans sa poche, un ordinateur aussi puissant que celui que tu viens de mettre 100 ans à construire !

---

L'autre penchant de la lois de Moore c'est que si on construit des ordinateurs qui font toujours la même taille, ils doublent en puissance tous les deux ans. Et du coup, on peut légitimement se poser la question : Si les ordinateurs sont toujours de plus en plus puissants, pourquoi est ce que ils rament toujours autant ? Et ça c'est très bien expliqué par la loi de Wirth. La loi de wirth en gros ça dit : "plus les ordinateurs sont rapides, plus les programmes qu'on développe pour ces ordinateurs sont lents". Plus on a de puissance, plus on fait des programmes qui sont gourmands en puissance. Si la puissance des ordinateurs double tous les deux ans, la lenteur des programme doublera tous les deux ans.

Par exemple à la fin des années 90, on avait un logiciel qui s'appelais Microsoft Word, qui était un petit éditeur de texte très pratique, mais avec un gros défaut, il rammait beaucoup. Si on avait gardé ce logiciel tel quel, aujourd'hui il serait extrêmement rapide. Mais au lieu de ça, au fur et à mesure que les ordinateurs devenait plus puissant, on a ajouté des fonctionnalités à Microsoft Word, de telle sorte à ce qu'il soit toujours aussi lent. Et c'est comme ça, on y peut rien. Ce que nous dit la loi de Wirth, c'est que la lenteur est inérente à l'informatique tel qu'on le pratique.

Si vous avez connu les macs avant les années 2000, vous vous souvenez peut-être de ce très bel icône en forme de montre.

![La petite montre de Susan](img/reine-rouge-09.tiff)

C'était un curseur, qui avait été dessiné par Susan Kare. Et il apparaissait dès que l'ordinateur rammait. Et même si la montre était très bien dessinée et très jolie, les utilisateurs detestaient voir la petite montre de Susan. Et c'est normal car quand on attend quelque chose, il y a rien de plus agaçant que de regarder sa montre, ça fait paraitre le temps encore plus long.

![Utilisateur agaçé qui regarde sa montre](img/reine-rouge-10.tiff)

Quand Apple a sorti sa nouvelle génération de système d'exploitation (OSX), ils auraient pu se dire : "les gens detestent voir la petite montre de Susan, et nos ordis sont devenu plus puissants, on va s'arranger pour que les programmes ne ramment plus." Mais ils ne se sont pas du tout dit ça. Ils se sont dit: "Personne n'aime attendre en regardant sa montre, qu'à cela ne tienne, on va remplacer le joli dessin de Susan par un icône nettement moins joli. Une sorte de petite roue qui tourne, un peu en forme de spirale...

![La roue qui tourne en forme de spirale](img/reine-rouge-11.tiff)

Comme ça quand les ordinateurs rameront, ça va un peu hypnotiser les gens et ils ne vont pas voir le temps passer."

![Utilisateur hypnotisé](img/reine-rouge-10.tiff)

C'était un choix délibéré. Sous aucun pretexte apple n'aurait fait des programmes qui rament moins. Parce qu'il faut bien comprendre quelquechose : Le jour où les programmes ne rament plus, il n'y a plus besoin de faire des nouveaux ordinateurs plus puissants. Et si il n'y a pas d'ordi plus puissant, aucune raison de faire des nouveaux programme plus gourmands. Et s'il n'y a plus besoin de faire des nouveaux ordinateurs ni de faire des nouveaux programmes, c'est tout le modèle économique d'Apple s'éfondre.

En fait, l'histoire de l'informatique, c'est le syndrome de la reine rouge.

![Alice et la Reine Rouge](img/reine-rouge-10.tiff)

Je sais pas si vous vous souvenez dans *Alice au pays des merveille*, il y a ce passage où Alice et la Reine Rouge courent pendant très longtemps. Elles courent dans une genre de forêt. Et c'est très fatiguant pour Alice. Parce que la reine court vraiment vite et aussi ça fait vraiment longtemps qu'elles courent.

À un moment elles décident de faire une pause et Alice s'apperçoit qu'elles n'ont pas bougé de l'endroit où elles étaient au début de la course. Elles sont toujours devant le même arbre qu'au début. Et donc Alice elle dit à la reine: "C'est bizarre, là d'où je viens, quand on court beaucoup comme ça, on change d'endroit." Et la reine rouge lui répond : "Ça doit être très lent là d'où tu viens. Ici il faut courrir de toutes ses forces, si on veut rester au même endroit."

Et pour moi l'histoire de l'informatique c'est vraiment ça, on court à des vitesses folles, et c'est comme si on avançait pas. On a des débits internet super faibles et on galère à regarder des vidéos en 720p. 10 ans plus tard, on a tous la fibre, les débits ont été multiptiés par 10, et nous qu'est-ce qu'on fait ? Au lieu de profiter de vidéos fluides en 720p, on galère à regarder des vidéos en 4K. On a des disques durs de quelques mégaoctets qui sont toujours "presque plein", 20 ans plus tard, on a des disques durs de plusieurs téraoctets et ils sont toujours "presque plein".

Et plus on court, et plus on reste au même endroit.

---

Un autre exemple que j'aime bien du syndrôme de la Reine Rouge dans l'histoire de l'informatique, c'est l'histoire du bug de l'an 2000.

Le bug de l'an 2000 on a pris l'habitude de l'écrire comme ça : Y2K BUG (Le Y ça veut dire année, et le K ça veut dire 1000 c'est le K de kilo). Et donc le bug de l'an 2000 c'est un bug qui n'a pas eu lieu. Il devait se produire au 1er Janvier 2000 un peu partout dans le monde. Et s'il n'a pas eu lieu c'est parce qu'on a vraiment eu peur et qu'on a dépensé des centaines de milliards de dollars pour le corriger avant qu'il ne se produise.

Le problème qu'on avait est assez simple à comprendre : Quand on faisait des programmes dans les années 80 et qu'il fallait garder la date en mémoire, on n'écrivait pas "1980", (ça prenais trop de place) on écrivait seulement "80". Et au moment de réstituer l'information on ajoutais "19" devant.

![19[80]](img/reine-rouge-10.tiff)

À cette époque les gens se disait que c'était cool, ça économisais pas mal de la mémoire. Ils étaient pas bêtes, ils savaient bien que ça pourrait pas durer eternellement. Mais ils étaient tous d'accord pour dire que les programme des années 80, ça fera longtemps qu'ils ne seraient plus utilisés dans les années 2000.


Sauf que 19 ans plus tard on est en 99, et les programmes des années 80 sont encore utilisé un peu partout dans le monde. Sauf que bientôt on va passer à l'an 2000. Et selon comment les programmes sont fait, soit ils vont nous ramener en 1900, soit (encore pire) nous enmener en 19100 ! Et croyez moi, ça aurait eu des conséquences assez graves.


![19[99] -> 19[00] / 19[100]](img/reine-rouge-10.tiff)

Heureusement on a réagit à temps. Et surtout on a appris de nos erreurs. Maintenant c'est plus du tout comme ça qu'on stocke les dates en mémoire. Maintenant pour stocker la date on utilise généralement ce qui s'appelle le temps UNIX. Le temps UNIX C'est le nombre de secondes qui se sont écoulés depuis le 1er janvier 1970. Par exemple , au moment ou j'écris ce texte, le temps UNIX c'est 1 666 012 960.

Le temps Unix c'est super, comme c'est un nombre de secondes c'est facile à convertir en nombre d'années et surtout, ça ne pose pas de problèmes quand on change de sciècle ou de milénaire !

Et cette valeur, on va la stocker sur 4 octets (c'est à dire 4 mains à 8 doigts qui comptent en binaire) et comme ça on est tranquille pour un bon bout de temps. Plus précisément on est tranquille jusqu'en 2038. Et ça va 2038 C'est dans longtemps. De toute façon, les programmes qu'on a aujourd'hui, ça fera longtemps qu'ils seront plus utilisés en 2038...

Je rigole bien sûr, c'est problématique et on est au courant. Ça s'appelle le bug de l'an 2038 et c'est pris très au serieux. On a pris l'habitude de l'écrire comme ça : Y2K38 BUG.

Rapidement, l'idée pour fixer ce bug, c'est qu'au lieu d'utiliser 4 octets, on en utilise 8. Et si on stock le temps unix sur 8 octets, le prochain bug de ce type il se produira dans 292 milliards d'années. Et ça c'est dans vraiment longtemps. C'est ce qu'on appelle le bug de l'an 292 milliards. On l'écrit comme ça : Y292G BUG (Le G c'est ça veut dire milliard, c'est le G de giga).

Et même si c'est dans vraiment longtemps, (l'an 292 milliards c'est bien après la disparition de la terre et du soleil) c'est important de préciser que ce bug, cette limitation, elle existe quand même.  Parce que ça rejoint la question que vous me posiez au départ : "jusqu'à combien je sais compter".

Parce que comme je disais au départ, personne ne peut prétendre compter jusqu'à l'infini. Et même pas les ordinateurs. Enfait je peux aller plus loin, les ordinateurs, même s'ils comptent plus vite et plus loin que nous. Ils peuvent encore moins compter jusqu'à l'infini que nous.

Parce que compter jusqu'à l'infini c'est pas une histoire de compter vite. Tu peux être le processeur le plus rapide du monde et avoir une cadence de 50Ghz, c'est à dire être capable compter 50 milliards de nombres par secondes. Tu peux compter de 10 en 10, de 1000 en 1000, tu peux même compter de plus en plus vite sans jamais arrêter d'accélerer. Quoi que tu fasse, l'infini il va pas se rapprocher pour autant.

Parce que compter jusqu'à l'infini c'est comme courrir avec la Reine Rouge : Tu peux courrir de toutes tes forces, aussi longtemps que tu veux, tu sera toujours au même endroit, toujours aussi loin de ton but.